{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLGeometry guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This introduction demonstrates how to use MLGeometry to: \n",
    "1. Generate a hypersurface.\n",
    "2. Build a bihomogeneous neural network.\n",
    "3. Use the model to compute numerical Calabi-Yau metrics with the embedding method.\n",
    "4. Plot $\\eta$ on a rational curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tensorflow_probability to use the L-BFGS optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import sympy as sp\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.keras.backend as K\n",
    "import tensorflow_probability as tfp"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "\n",
    "from src.MLGeometry import bihomoNN as bnn"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries to plot the $\\eta$ on the rational curve (see the last section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a random seed (optional)\n",
    "Some random seed might be bad for numerical calulations. If there are any errors during the training, you may want to try a different seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a hypersurface\n",
    "First define a set of coordinates and a function as sympy symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "z0, z1, z2, z3, z4 = sp.symbols('z0, z1, z2, z3, z4')\n",
    "Z = [z0,z1,z2,z3,z4]\n",
    "f = z0**5 + z1**5 + z2**5 + z3**5 + z4**5 + 0.5*z0*z1*z2*z3*z4"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define a hypersurface as a collection of points which solve the equation f = 0, using the `Hypersurface` class in the `mlg.hypersurface` module. The parameter n_pairs is the number of random pairs of points used to form the random lines in $\\mathbf{CP}^{N+1}$. Then we take the intersections of those random lines and the hypersurface. By Bezout's theorem, each line intersects the hypersurface in precisely d points where d is the number of homogeneous coordinates. So the total number of points is d * n_pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "n_pairs = 10000\n",
    "HS_train = src.MLGeometry.hypersurface.Hypersurface(Z, f, n_pairs)\n",
    "HS_test = src.MLGeometry.hypersurface.Hypersurface(Z, f, n_pairs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hypersurface class will take care of the patchwork automatically. Let's use the `list_patches` function to check the number of points on each patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "HS_train.list_patches()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also invoke this method on one of the patches to check the distribution on the subpatches:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "HS_train.patches[0].list_patches()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hypersurface class contains some symbolic and numerical methods as well, which will be introduced elsewhere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Tensorflow\n",
    "The following steps are similar to a regular Tensorflow training process. \n",
    "### Generate datasets\n",
    "The `mlg.tf_dataset.generate_dataset` function converts a hypersurface to a Tensorflow Dataset, which has four componets: the points on the hypersurface, the volume form $\\small \\Omega \\wedge \\bar\\Omega$, the mass reweighting the points distribution and the restriction which restricts the Kähler metric to a subpatch. The restriction contains an extra linear transformation so that points on different affine patches can all be processed in one call. It is also possible to generate a dataset only on one affine patch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "train_set = src.MLGeometry.tf_dataset.generate_dataset(HS_train)\n",
    "test_set = src.MLGeometry.tf_dataset.generate_dataset(HS_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle and batch the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "train_set = train_set.shuffle(HS_train.n_points).batch(1000)\n",
    "test_set = test_set.shuffle(HS_test.n_points).batch(1000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what is inside a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "points, Omega_Omegabar, mass, restriction = next(iter(train_set))\n",
    "print(points)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a bihomogeneous neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mlg.bihomoNN` module provides the necessary layers (e.g. `Bihomogeneous` and `Dense` ) to construct the Kähler potential with a bihomogeneous neural network. Here is an example of a two-hidden-layer network (k = 4) with 70 and 100 hidden units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "class Kahler_potential(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Kahler_potential, self).__init__()\n",
    "        # The first layer transforms the complex points to the bihomogeneous form.\n",
    "        # The number of the outputs is d^2, where d is the number of coordinates.\n",
    "        self.bihomogeneous = bnn.Bihomogeneous(d=5)\n",
    "        self.layer1 = bnn.SquareDense(5**2, 70, activation=tf.square)\n",
    "        self.layer2 = bnn.SquareDense(70, 100, activation=tf.square)\n",
    "        self.layer3 = bnn.SquareDense(100, 1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.bihomogeneous(inputs)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = tf.math.log(x)\n",
    "        return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "model = Kahler_potential()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Kähler metric $g_{i \\bar j} = \\partial_i\\bar\\partial_{\\bar j} K$ and the volume form $d\\mu_g = \\det g_{i \\bar j}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "source": [
    "@tf.function\n",
    "def volume_form(points, Omega_Omegabar, mass, restriction):\n",
    "    \n",
    "    kahler_metric = src.MLGeometry.complex_math.complex_hessian(tf.math.real(model(points)), points)\n",
    "    volume_form = tf.matmul(restriction, tf.matmul(kahler_metric, restriction, adjoint_b=True))\n",
    "    volume_form = tf.math.real(tf.linalg.det(volume_form))\n",
    "    \n",
    "    # Calculate the normalization constant to make the overall integration as 1\n",
    "    # It is a batchwise calculation but we expect it to converge to a constant eventually\n",
    "    weights = mass / tf.reduce_sum(mass)\n",
    "    factor = tf.reduce_sum(weights * volume_form / Omega_Omegabar)\n",
    "    \n",
    "    return volume_form / factor"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with Adam and L-BFGS\n",
    "#### Adam\n",
    "Setup the keras optmizer as `Adam` and the loss function as one of weighted loss in the `mlg.loss` module. Some available functions are `weighted_MAPE`, `weighted_MSE`, `max_error` and `MAPE_plus_max_error`. They are weighted with the mass formula since the points on the hypersurface are distributed according to the Fubini-Study measure while the measure used in the integration is determined by the volume form $\\small \\Omega \\wedge \\bar\\Omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_func = src.MLGeometry.loss.weighted_MAPE"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the batches and train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "max_epochs = 500\n",
    "epoch = 0\n",
    "while epoch < max_epochs:\n",
    "    epoch = epoch + 1\n",
    "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(train_set):\n",
    "        with tf.GradientTape() as tape:\n",
    "            det_omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
    "            loss = loss_func(Omega_Omegabar, det_omega, mass)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"epoch %d: loss = %.5f\" % (epoch, loss))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the loss of the test dataset. First define a function to calculate the total loss over the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "def cal_total_loss(dataset, loss_function):\n",
    "    total_loss = tf.constant(0, dtype=tf.float32)\n",
    "    total_mass = tf.constant(0, dtype=tf.float32)\n",
    "    \n",
    "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(dataset):\n",
    "        det_omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
    "        mass_sum = tf.reduce_sum(mass)\n",
    "        total_loss += loss_function(Omega_Omegabar, det_omega, mass) * mass_sum\n",
    "        total_mass += mass_sum\n",
    "    total_loss = total_loss / total_mass\n",
    "\n",
    "    return total_loss.numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results of MAPE and MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "sigma_test = cal_total_loss(test_set, src.MLGeometry.loss.weighted_MAPE)\n",
    "E_test = cal_total_loss(test_set, src.MLGeometry.loss.weighted_MSE)\n",
    "print(\"sigma_test = %.5f\" % sigma_test)\n",
    "print(\"E_test = %.5f\" % E_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the error of the Monte Carlo integration, estimated by: \n",
    "\n",
    "$$\\delta \\sigma = \\frac{1}{\\sqrt{N_p}} {\\left( \\int_X (|\\eta - 1_X| - \\sigma)^2 d\\mu_{\\Omega}\\right)}^{1/2},$$\n",
    "\n",
    "where $N_p$ is the number of points on the hypersurface and $\\sigma$ is the `weighted_MAPE` loss, and \n",
    "\n",
    "$$\\eta = \\frac{\\det \\omega}{\\small \\Omega \\wedge \\bar \\Omega}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "def delta_sigma_square_test(y_true, y_pred, mass):\n",
    "    weights = mass / K.sum(mass)\n",
    "    return K.sum((K.abs(y_true - y_pred) / y_true - sigma_test)**2 * weights)\n",
    "\n",
    "delta_sigma = cal_total_loss(test_set, delta_sigma_square_test)\n",
    "print(\"delta_simga = %.5f\" % delta_sigma)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Load\n",
    "The trained network can be saved by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "model.save('trained_model/70_100_1')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And loaded by the `load_model` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "model = tf.keras.models.load_model('trained_model/70_100_1', compile=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-BFGS\n",
    "As elaborated in our paper, when the network getting more complicated, L-BFGS converges faster than Adam near the minima. It is recommanded to use it after pretraining with Adam. However, L-BFGS is not in the standard Tensorflow library so the training process is slightly different: (Only ~20 iterations are shown here. In a real case you may want to set the `max_epochs` to ~1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "source": [
    "# The displayed max_epochs will be three to four times this value since iter + 1 everytime the function\n",
    "# is invoked, which also happens during the evaluation of the function itself and its gradient\n",
    "max_epochs = 5\n",
    "\n",
    "# Setup the function to be optimized by L-BFGS\n",
    "\n",
    "train_func = src.MLGeometry.lbfgs.function_factory(model, loss_func, train_set)\n",
    "\n",
    "# Setup the inital values and train\n",
    "init_params = tf.dynamic_stitch(train_func.idx, model.trainable_variables)\n",
    "results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func,\n",
    "                                        initial_position=init_params,\n",
    "                                        max_iterations=max_epochs)\n",
    "# Update the model after the last loop\n",
    "train_func.assign_new_model_parameters(results.position)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the definition of the volume form is already in the `mlg.lbfgs` module. Also note that the standard L-BFGS does not support multi-batch training. You can still batch the dataset in case the GPU is out of memory, but the parameters are only updated after a whole epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "sigma_test = cal_total_loss(test_set, src.MLGeometry.loss.weighted_MAPE)\n",
    "E_test = cal_total_loss(test_set, src.MLGeometry.loss.weighted_MSE)\n",
    "print(\"sigma_test = %.5f\" % sigma_test)\n",
    "print(\"E_test = %.5f\" % E_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the metrics\n",
    "After all of the trainings are done, the final results for the metrics can be printed out explicitly, using the previously generated data points and restriction matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "@tf.function\n",
    "def get_cy_metric(points, restriction):\n",
    "    \n",
    "    cy_metric = src.MLGeometry.complex_math.complex_hessian(tf.math.real(model(points)), points)\n",
    "    cy_metric = tf.matmul(restriction, tf.matmul(cy_metric, restriction, adjoint_b=True))\n",
    "    \n",
    "    return cy_metric"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "source": [
    "cy_metric = get_cy_metric(points, restriction)\n",
    "print(points[5].numpy())\n",
    "print(cy_metric[5].numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\eta$ on the rational curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's retrict our model to a subspace and check the local behavior of $\\eta$. With the quintic 3-fold f = 0, we can choose the embedding \n",
    "\n",
    "$$(z_0, -z_0, z_1, 0, -z_1),$$\n",
    "\n",
    "and the local coordinate system defined by $t = z_1 / z_0$. Using shperical coordinates $(\\theta, \\phi)$, it can be embedded into $\\mathbb{R}^3$ by:\n",
    "\n",
    "$$z_0 = \\sin \\theta \\cos \\phi, \\qquad z_1= \\sin \\theta \\sin \\phi + i \\cos \\phi$$\n",
    "\n",
    "So first sample the points on the rational curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "source": [
    "theta, phi = np.linspace(0.001,np.pi+0.001, 400), np.linspace(0.001, 2*np.pi+0.001, 400)\n",
    "eps = 0.0001 + 0.0001j\n",
    "\n",
    "R = []\n",
    "points_list = []\n",
    "for j in phi:\n",
    "    for i in theta:\n",
    "        t = complex(math.sin(i)*math.sin(j), math.cos(i)) / (math.sin(i)*math.cos(j))\n",
    "        if np.absolute(t) <= 1:\n",
    "            # The Bihomogeneous layer will remove the zero entries automatically.\n",
    "            # So here we add a small number eps to avoid being removed\n",
    "            points_list.append([1+eps, -1+eps, t+eps, 0+eps, -t+eps])\n",
    "        else:\n",
    "            # Use the symmetry:\n",
    "            points_list.append([1+eps, -1+eps, 1/t+eps, 0+eps, -1/t+eps])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this set of points to generate the rational curve with norm_coordinate = z0 and max_grad_coordinate = z1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "source": [
    "rc = src.MLGeometry.hypersurface.Hypersurface(Z, f, points=points_list, norm_coordinate=0, max_grad_coordinate=0)\n",
    "rc_dataset = src.MLGeometry.tf_dataset.generate_dataset(rc).batch(rc.n_points)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $\\eta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "source": [
    "points, Omega_Omegabar, mass, restriction = next(iter(rc_dataset))\n",
    "det_omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
    "eta = (det_omega / Omega_Omegabar).numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to Cartesian coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "R = eta.reshape(400, 400)\n",
    "THETA, PHI = np.meshgrid(theta, phi)\n",
    "X = R * np.sin(THETA) * np.cos(PHI)\n",
    "Y = R * np.sin(THETA) * np.sin(PHI)\n",
    "ZZ = R * np.cos(THETA)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "ax.set_zlim3d(-1.0, 1.0)\n",
    "plot = ax.plot_surface(\n",
    "    X, Y, ZZ, rstride=1, cstride=1, cmap=plt.cm.YlGnBu_r,\n",
    "    linewidth=0, antialiased=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta$ is expected to approach the constant function 1 as k increases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
